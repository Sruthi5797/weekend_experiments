# Pickup_Cupid

Welcome to **Pickup_Cupid**, an innovative project that leverages large language models (LLMs) to generate charming and witty pickup lines. This project uses Streamlit for a user-friendly web interface and FastAPI for a robust backend API.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [API Endpoints](#api-endpoints)
- [Contributing](#contributing)
- [License](#license)

## Overview

**Pickup_Cupid** is designed to generate creative and fun pickup lines using advanced natural language processing techniques. The project aims to provide a seamless and engaging experience through a web application built with Streamlit and an API powered by FastAPI.

## Features

- **User-Friendly Interface**: An interactive web app built with Streamlit.
- **API Access**: A FastAPI backend for easy integration with other applications.
- **Customizable Lines**: Generate pickup lines based on various themes and keywords.
- **Real-Time Generation**: Fast and responsive generation of pickup lines.

## Model used

## Installation

### Prerequisites

- Python 3.8 or higher
- pip (Python package installer)

### Steps


1. Clone the repository:

   ```sh
   git clone https://github.com/yourusername/pickup_cupid.git
   cd pickup_cupid

2. Create a Virtual environment
- python -m venv venv
- source venv/bin/activate  # On Windows use `venv\Scripts\activate`

3. Install dependencies

- pip install -r requirements.txt

4. Run the application

- streamlit run app.py
- uvicorn main:app --reload

5. Alternate docker container


### Ollama server for llama3

1. Ollama installation:

- follow the instructions to setup llama in your local machine - (Link)[https://github.com/ollama/ollama]


### License

### Resources

### Contributions
